<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Human Pose Estimation using BLIP | Abdullah Azhar</title>
<meta name=keywords content><meta name=description content="Jinxuan Liang, Yihua Zhou, Abdullah Azhar, Anjana Manjunath University of California, Berkeley"><meta name=author content="Abdullah Azhar"><link rel=canonical href=http://localhost:1313/publications-and-preprints/blip/><link crossorigin=anonymous href=/assets/css/stylesheet.549e408c1cdbdced3c2cac56525067992a513db204d11fd97bfa6070cf92a519.css integrity="sha256-VJ5AjBzb3O08LKxWUlBnmSpRPbIE0R/Ze/pgcM+SpRk=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.png><link rel=mask-icon href=http://localhost:1313/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/publications-and-preprints/blip/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q603T56FWT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q603T56FWT",{anonymize_ip:!1})}</script><meta property="og:title" content="Human Pose Estimation using BLIP"><meta property="og:description" content="Jinxuan Liang, Yihua Zhou, Abdullah Azhar, Anjana Manjunath University of California, Berkeley"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/publications-and-preprints/blip/"><meta property="og:image" content="http://localhost:1313/favicon.png"><meta property="article:section" content="publications-and-preprints"><meta property="article:published_time" content="2024-03-28T17:41:00-07:00"><meta property="article:modified_time" content="2024-03-28T17:41:00-07:00"><meta property="og:site_name" content="Abdullah Azhar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/favicon.png"><meta name=twitter:title content="Human Pose Estimation using BLIP"><meta name=twitter:description content="Jinxuan Liang, Yihua Zhou, Abdullah Azhar, Anjana Manjunath University of California, Berkeley"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Publications and Preprints","item":"http://localhost:1313/publications-and-preprints/"},{"@type":"ListItem","position":2,"name":"Human Pose Estimation using BLIP","item":"http://localhost:1313/publications-and-preprints/blip/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Human Pose Estimation using BLIP","name":"Human Pose Estimation using BLIP","description":"Jinxuan Liang, Yihua Zhou, Abdullah Azhar, Anjana Manjunath University of California, Berkeley","keywords":[],"articleBody":"FINE-TUNING VISION TRANSFORMER-BASED MODEL FOR POSE-ESTIMATION Abstract: With the development of the Computer Vision discipline within Deep Learning, Pose Estimation tasks have seen increasing interest and growth in the past decade. Pose Estimation refers to the problem of localizing specific body parts in images and videos and encoding the spatial information into a caption relating human skeletal joints with pixel coordinates. While Pose Estimation models have largely been Convolutional Neural Network (CNN)-based, the advent of Vision Transformers (ViT) has opened new areas of research. In this study, we fine-tune a transformer-based model, BLIP, to generate accurate positional captions and explore attention mechanisms under this task. Starting with a low-complexity version of our problem, we scaled up to our task of fine-tuning BLIP for Pose Estimation. After fine-tuning BLIP on varying hyperparameters, we found that the model consistently performed well during tuning. Within a restricted threshold of 1 pixel, it retained an 81% validation accuracy and an average error of about 4 pixels. Link to Preprint Code ","wordCount":"166","inLanguage":"en","datePublished":"2024-03-28T17:41:00-07:00","dateModified":"2024-03-28T17:41:00-07:00","author":{"@type":"Person","name":"Abdullah Azhar"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/publications-and-preprints/blip/"},"publisher":{"@type":"Organization","name":"Abdullah Azhar","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Abdullah Azhar (Alt + H)"><img src=http://localhost:1313/favicon.png alt="Site icon in header" aria-label=logo height=35>Abdullah Azhar</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/education/ title=Education><span>Education</span></a></li><li><a href=http://localhost:1313/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1313/publications-and-preprints/ title="Papers & Preprints"><span>Papers & Preprints</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;Â»&nbsp;<a href=http://localhost:1313/publications-and-preprints/>Publications and Preprints</a></div><h1 class=post-title>Human Pose Estimation using BLIP</h1></header><div class=post-content><h4 id=fine-tuning-vision-transformer-based-model-for-pose-estimation>FINE-TUNING VISION TRANSFORMER-BASED MODEL FOR POSE-ESTIMATION<a hidden class=anchor aria-hidden=true href=#fine-tuning-vision-transformer-based-model-for-pose-estimation>#</a></h4><h5 id=abstract>Abstract:<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h5><h5 id=with-the-development-of-the-computer-vision-discipline-within-deep-learning-pose-estimation-tasks-have-seen-increasing-interest-and-growth-in-the-past-decade-pose-estimation-refers-to-the-problem-of-localizing-specific-body-parts-in-images-and-videos-and-encoding-the-spatial-information-into-a-caption-relating-human-skeletal-joints-with-pixel-coordinates-while-pose-estimation-models-have-largely-been-convolutional-neural-network-cnn-based-the-advent-of-vision-transformers-vit-has-opened-new-areas-of-research-in-this-study-we-fine-tune-a-transformer-based-model-blip-to-generate-accurate-positional-captions-and-explore-attention-mechanisms-under-this-task-starting-with-a-low-complexity-version-of-our-problem-we-scaled-up-to-our-task-of-fine-tuning-blip-for-pose-estimation-after-fine-tuning-blip-on-varying-hyperparameters-we-found-that-the-model-consistently-performed-well-during-tuning-within-a-restricted-threshold-of-1-pixel-it-retained-an-81-validation-accuracy-and-an-average-error-of-about-4-pixels>With the development of the Computer Vision discipline within Deep Learning, Pose Estimation tasks have seen increasing interest and growth in the past decade. Pose Estimation refers to the problem of localizing specific body parts in images and videos and encoding the spatial information into a caption relating human skeletal joints with pixel coordinates. While Pose Estimation models have largely been Convolutional Neural Network (CNN)-based, the advent of Vision Transformers (ViT) has opened new areas of research. In this study, we fine-tune a transformer-based model, BLIP, to generate accurate positional captions and explore attention mechanisms under this task. Starting with a low-complexity version of our problem, we scaled up to our task of fine-tuning BLIP for Pose Estimation. After fine-tuning BLIP on varying hyperparameters, we found that the model consistently performed well during tuning. Within a restricted threshold of 1 pixel, it retained an 81% validation accuracy and an average error of about 4 pixels.<a hidden class=anchor aria-hidden=true href=#with-the-development-of-the-computer-vision-discipline-within-deep-learning-pose-estimation-tasks-have-seen-increasing-interest-and-growth-in-the-past-decade-pose-estimation-refers-to-the-problem-of-localizing-specific-body-parts-in-images-and-videos-and-encoding-the-spatial-information-into-a-caption-relating-human-skeletal-joints-with-pixel-coordinates-while-pose-estimation-models-have-largely-been-convolutional-neural-network-cnn-based-the-advent-of-vision-transformers-vit-has-opened-new-areas-of-research-in-this-study-we-fine-tune-a-transformer-based-model-blip-to-generate-accurate-positional-captions-and-explore-attention-mechanisms-under-this-task-starting-with-a-low-complexity-version-of-our-problem-we-scaled-up-to-our-task-of-fine-tuning-blip-for-pose-estimation-after-fine-tuning-blip-on-varying-hyperparameters-we-found-that-the-model-consistently-performed-well-during-tuning-within-a-restricted-threshold-of-1-pixel-it-retained-an-81-validation-accuracy-and-an-average-error-of-about-4-pixels>#</a></h5><h5 id=link-to-preprintfinal_paperpdf><a href=final_paper.pdf>Link to Preprint</a><a hidden class=anchor aria-hidden=true href=#link-to-preprintfinal_paperpdf>#</a></h5><h5 id=codehttpsgithubcomrichzhou1999cs282_final_project_codebase><a href=https://github.com/RichZhou1999/cs282_final_project_codebase>Code</a><a hidden class=anchor aria-hidden=true href=#codehttpsgithubcomrichzhou1999cs282_final_project_codebase>#</a></h5></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://github.com/azhara001 rel=me title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=https://www.linkedin.com/in/azhara001/ rel=me title=Linkedin data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a style=border-bottom:none href=mailto:abdullah_azhar@berkeley.edu rel=me title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2024 <a href=http://localhost:1313/>Abdullah Azhar</a></span>
<span>â¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>