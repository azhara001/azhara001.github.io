<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fine Tuning BLIP | Abdullah Azhar</title>
<meta name=keywords content><meta name=description content="Fine-Tuning BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation) Vision-based Transformer Model For Human-Pose Estimation"><meta name=author content="Abdullah Azhar"><link rel=canonical href=https://azhara001.github.io/projects/fine-tuning-blip/><link crossorigin=anonymous href=/assets/css/stylesheet.549e408c1cdbdced3c2cac56525067992a513db204d11fd97bfa6070cf92a519.css integrity="sha256-VJ5AjBzb3O08LKxWUlBnmSpRPbIE0R/Ze/pgcM+SpRk=" rel="preload stylesheet" as=style><link rel=icon href=https://azhara001.github.io/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://azhara001.github.io/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://azhara001.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://azhara001.github.io/favicon.png><link rel=mask-icon href=https://azhara001.github.io/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://azhara001.github.io/projects/fine-tuning-blip/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q603T56FWT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q603T56FWT",{anonymize_ip:!1})}</script><meta property="og:title" content="Fine Tuning BLIP"><meta property="og:description" content="Fine-Tuning BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation) Vision-based Transformer Model For Human-Pose Estimation"><meta property="og:type" content="article"><meta property="og:url" content="https://azhara001.github.io/projects/fine-tuning-blip/"><meta property="og:image" content="https://azhara001.github.io/favicon.png"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-03-28T14:19:50-07:00"><meta property="article:modified_time" content="2024-03-28T14:19:50-07:00"><meta property="og:site_name" content="Abdullah Azhar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://azhara001.github.io/favicon.png"><meta name=twitter:title content="Fine Tuning BLIP"><meta name=twitter:description content="Fine-Tuning BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation) Vision-based Transformer Model For Human-Pose Estimation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://azhara001.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Fine Tuning BLIP","item":"https://azhara001.github.io/projects/fine-tuning-blip/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine Tuning BLIP","name":"Fine Tuning BLIP","description":"Fine-Tuning BLIP (Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation) Vision-based Transformer Model For Human-Pose Estimation","keywords":[],"articleBody":" Fine-Tuning BLIP for Pose Estimation\nLinks: Preprint Poster Github Abstract: With the development of the Computer Vision discipline within Deep Learning, Pose Estimation tasks have seen increasing interest and growth in the past decade. Pose Estimation refers to the problem of localizing specific body parts in images and videos and encoding the spatial information into a caption relating human skeletal joints with pixel coordinates. While Pose Estimation models have largely been Convolutional Neural Network (CNN)-based, the advent of Vision Transformers (ViT) has opened new areas of research. In this study, we fine-tune a transformer-based model, BLIP, to generate accurate positional captions and explore attention mechanisms under this task. Starting with a low-complexity version of our problem, we scaled up to our task of fine-tuning BLIP for Pose Estimation. After fine-tuning BLIP on varying hyperparameters, we found that the model consistently performed well during tuning. Within a restricted threshold of 1 pixel, it retained an 81% validation accuracy and an average error of about 4 pixels.\n","wordCount":"165","inLanguage":"en","datePublished":"2024-03-28T14:19:50-07:00","dateModified":"2024-03-28T14:19:50-07:00","author":{"@type":"Person","name":"Abdullah Azhar"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://azhara001.github.io/projects/fine-tuning-blip/"},"publisher":{"@type":"Organization","name":"Abdullah Azhar","logo":{"@type":"ImageObject","url":"https://azhara001.github.io/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://azhara001.github.io/ accesskey=h title="Abdullah Azhar (Alt + H)"><img src=https://azhara001.github.io/favicon.png alt="Site icon in header" aria-label=logo height=35>Abdullah Azhar</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://azhara001.github.io/about/ title=About><span>About</span></a></li><li><a href=https://azhara001.github.io/education/ title=Education><span>Education</span></a></li><li><a href=https://azhara001.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://azhara001.github.io/publications-and-preprints/ title="Papers & Preprints"><span>Papers & Preprints</span></a></li><li><a href=https://azhara001.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://azhara001.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://azhara001.github.io/projects/>Projects</a></div><h1 class=post-title>Fine Tuning BLIP</h1></header><div class=post-content><img style=margin-left:auto;margin-right:auto width=50% src=img/BLIP_img.png alt="Fine-Tuning BLIP for Pose Estimation"><p align=center>Fine-Tuning BLIP for Pose Estimation</p><h2 id=links>Links:<a hidden class=anchor aria-hidden=true href=#links>#</a></h2><ul><li><a href=img/final_paper.pdf>Preprint</a></li><li><a href=img/poster.pdf>Poster</a></li><li><a href=https://github.com/RichZhou1999/cs282_final_project_codebase>Github</a></li></ul><h2 id=abstract>Abstract:<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><p>With the development of the Computer Vision discipline within Deep Learning, Pose Estimation tasks
have seen increasing interest and growth in the past decade. Pose Estimation refers to the problem
of localizing specific body parts in images and videos and encoding the spatial information into a
caption relating human skeletal joints with pixel coordinates. While Pose Estimation models have
largely been Convolutional Neural Network (CNN)-based, the advent of Vision Transformers (ViT)
has opened new areas of research. In this study, we fine-tune a transformer-based model, BLIP, to
generate accurate positional captions and explore attention mechanisms under this task. Starting
with a low-complexity version of our problem, we scaled up to our task of fine-tuning BLIP for Pose
Estimation. After fine-tuning BLIP on varying hyperparameters, we found that the model consistently
performed well during tuning. Within a restricted threshold of 1 pixel, it retained an 81% validation
accuracy and an average error of about 4 pixels.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://github.com/azhara001 rel=me title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=https://www.linkedin.com/in/azhara001/ rel=me title=Linkedin data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a style=border-bottom:none href=mailto:abdullah_azhar@berkeley.edu rel=me title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</a><a style=border-bottom:none href="https://drive.google.com/file/d/1MBMWhDP5oKR0UpoClEkJ4jrMEiAik2RC/view?usp=sharing" rel=me title=Resume><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></a></div><span>&copy; 2024 <a href=https://azhara001.github.io/>Abdullah Azhar</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>