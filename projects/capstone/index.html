<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning | Abdullah Azhar</title>
<meta name=keywords content><meta name=description content="Keywords - Deep Learning, Multi-Modal, Image-to-Text, Multiheaded Attention, Encoder-Decoder, PyTorch, Hugging Face, GPU, SGD, AdamW, Levenshtein Distance, Cross Entropy Loss, Attention Mask"><meta name=author content="Abdullah Azhar"><link rel=canonical href=https://azhara001.github.io/projects/capstone/><link crossorigin=anonymous href=/assets/css/stylesheet.549e408c1cdbdced3c2cac56525067992a513db204d11fd97bfa6070cf92a519.css integrity="sha256-VJ5AjBzb3O08LKxWUlBnmSpRPbIE0R/Ze/pgcM+SpRk=" rel="preload stylesheet" as=style><link rel=icon href=https://azhara001.github.io/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://azhara001.github.io/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://azhara001.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://azhara001.github.io/favicon.png><link rel=mask-icon href=https://azhara001.github.io/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://azhara001.github.io/projects/capstone/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q603T56FWT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q603T56FWT",{anonymize_ip:!1})}</script><meta property="og:title" content="Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning"><meta property="og:description" content="Keywords - Deep Learning, Multi-Modal, Image-to-Text, Multiheaded Attention, Encoder-Decoder, PyTorch, Hugging Face, GPU, SGD, AdamW, Levenshtein Distance, Cross Entropy Loss, Attention Mask"><meta property="og:type" content="article"><meta property="og:url" content="https://azhara001.github.io/projects/capstone/"><meta property="og:image" content="https://azhara001.github.io/favicon.png"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-06-05T14:18:23-07:00"><meta property="article:modified_time" content="2024-06-05T14:18:23-07:00"><meta property="og:site_name" content="Abdullah Azhar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://azhara001.github.io/favicon.png"><meta name=twitter:title content="Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning"><meta name=twitter:description content="Keywords - Deep Learning, Multi-Modal, Image-to-Text, Multiheaded Attention, Encoder-Decoder, PyTorch, Hugging Face, GPU, SGD, AdamW, Levenshtein Distance, Cross Entropy Loss, Attention Mask"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://azhara001.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning","item":"https://azhara001.github.io/projects/capstone/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning","name":"Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning","description":"Keywords - Deep Learning, Multi-Modal, Image-to-Text, Multiheaded Attention, Encoder-Decoder, PyTorch, Hugging Face, GPU, SGD, AdamW, Levenshtein Distance, Cross Entropy Loss, Attention Mask","keywords":[],"articleBody":" [ Presentation, Demo, Github, Website, Report ] ğŸ“ Completed a final CAPSTONE project in collaboration with Refiberd, working alongside an exceptional team including Isidora Rollan , Erin Jones, Mustafa Hameed , and Prashant Sharma. ğŸ”§ğŸ§© Prototyped an image-to-text captioning system for Refiberd projecting reduced workforce requirements for label collection by 50%, thereby saving 1,500 annual work hours. ğŸ’¡ Conceptualized, tested, and finalized the ML model, followed by fine-tuning and deployment preparation. ğŸ§ ğŸ’» Optimized the state-of-the-art multimodal encoder-decoder DONUT model, achieving an exceptional normalized Levenshtein distance of 0.005. ğŸ—„ï¸ğŸ“Š Developed a custom dataset by sourcing raw images of vendor tags and converting them into the Hugging Face Apache Arrow format, enhancing model training efficiency. ğŸ—ï¸ğŸ“Š Assembled Train Dataset: 469 Images ğŸ”§ğŸ“Š Tuned Validation Dataset: 112 Images ğŸ¯ğŸ“Š Evaluated Test Dataset: 66 Images. ğŸ” Developed a custom PyTorch training loop, logging key metrics such as train loss, validation accuracy, and test accuracy, while also synchronizing model states and checkpoints with the Hugging Face Hub. âš™ï¸ Explored and tuned hyperparameters, experimenting with various optimizers including SGD, SGD with momentum, and second-order optimizers like Adam and AdamW to enhance model performance. ğŸ–¥ï¸ Leveraged L4 GPU architecture to enhance training loops, efficiently managing GPU resources for improved model training workflows. ğŸ† Achieved a perfect match on 47 out of 66 test images, demonstrating a Levenshtein score of 0 and underscoring the model's robustness. âš–ï¸ Attained a Cross Entropy Loss of just 0.0005 on the training dataset for the decoder's next-token-prediction task, indicating high predictive accuracy. ğŸ” Currently developing an attention mask using output_state and BertViz for the decoder side to enhance insight into the attention mechanisms within the model. ","wordCount":"274","inLanguage":"en","datePublished":"2024-06-05T14:18:23-07:00","dateModified":"2024-06-05T14:18:23-07:00","author":{"@type":"Person","name":"Abdullah Azhar"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://azhara001.github.io/projects/capstone/"},"publisher":{"@type":"Organization","name":"Abdullah Azhar","logo":{"@type":"ImageObject","url":"https://azhara001.github.io/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://azhara001.github.io/ accesskey=h title="Abdullah Azhar (Alt + H)"><img src=https://azhara001.github.io/favicon.png alt="Site icon in header" aria-label=logo height=35>Abdullah Azhar</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://azhara001.github.io/about/ title=About><span>About</span></a></li><li><a href=https://azhara001.github.io/education/ title=Education><span>Education</span></a></li><li><a href=https://azhara001.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://azhara001.github.io/publications-and-preprints/ title="Papers & Preprints"><span>Papers & Preprints</span></a></li><li><a href=https://azhara001.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://azhara001.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://azhara001.github.io/projects/>Projects</a></div><h1 class=post-title>Fine Tuning OCR Free Document Understanding Transformer for Image-to-Text Captioning</h1></header><div class=post-content>[ <a href="https://drive.google.com/file/d/19VMUAhH1dSgoHinURd1WH0rSfcaBzgJk/view?usp=sharing" style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Presentation</i></ins></a>, <a href="https://www.youtube.com/watch?v=LD6xWYuB1wI" style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Demo</i></ins></a>, <a href=https://github.com/azhara001/Fabric-Composition-Extraction style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Github</i></ins></a>, <a href=https://mims24-capstone-efficient-labeling.com/ style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Website</i></ins></a>, <a href=https://www.ischool.berkeley.edu/sites/default/files/sproject_attachments/final_report_-_final.pdf style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Report</i></ins></a> ]<div style="background-color:#e6e6fa;padding:10px;border:1px solid #ccc;margin:10px 0;position:relative;border-radius:20px"><ul><li style=color:#036>ğŸ“ Completed a final CAPSTONE project in collaboration with <a href=https://refiberd.com/ style=color:#800020 target=_blank><ins><i>Refiberd</i></ins></a>, working alongside an exceptional team including <a href=https://www.linkedin.com/in/isidora-rollan-zegers/ style=color:#800020 target=_blank><ins><i>Isidora Rollan </i></ins></a>, <a href=https://www.linkedin.com/in/erin-engle-jones/ style=color:#800020 target=_blank><ins><i>Erin Jones</i></ins></a>, <a href=https://www.linkedin.com/in/mustafahameed491/ style=color:#800020 target=_blank><ins><i>Mustafa Hameed </i></ins></a>, and <a href=https://www.linkedin.com/in/prashant3123/ style=color:#800020 target=_blank><ins><i>Prashant Sharma</i></ins></a>.</li><li style=color:#b60442>ğŸ”§ğŸ§© Prototyped an image-to-text captioning system for Refiberd projecting reduced workforce requirements for label collection by 50%, thereby saving 1,500 annual work hours.</li><li style=color:#036>ğŸ’¡ Conceptualized, tested, and finalized the ML model, followed by fine-tuning and deployment preparation.</li><li style=color:#036>ğŸ§ ğŸ’» Optimized the state-of-the-art multimodal encoder-decoder <a href=https://arxiv.org/abs/2111.15664 style=color:#800020 target=_blank><ins><i>DONUT</i></ins></a> model, achieving an exceptional normalized Levenshtein distance of 0.005.</li><li style=color:#036>ğŸ—„ï¸ğŸ“Š Developed a custom dataset by sourcing raw images of vendor tags and converting them into the Hugging Face Apache Arrow format, enhancing model training efficiency.</li><li style=color:#036>ğŸ—ï¸ğŸ“Š Assembled Train Dataset: 469 Images</li><li style=color:#036>ğŸ”§ğŸ“Š Tuned Validation Dataset: 112 Images</li><li style=color:#036>ğŸ¯ğŸ“Š Evaluated Test Dataset: 66 Images.</li><li style=color:#036>ğŸ” Developed a custom PyTorch training loop, logging key metrics such as train loss, validation accuracy, and test accuracy, while also synchronizing model states and checkpoints with the Hugging Face Hub.</li><li style=color:#036>âš™ï¸ Explored and tuned hyperparameters, experimenting with various optimizers including SGD, SGD with momentum, and second-order optimizers like Adam and AdamW to enhance model performance.</li><li style=color:#036>ğŸ–¥ï¸ Leveraged L4 GPU architecture to enhance training loops, efficiently managing GPU resources for improved model training workflows.</li><li style=color:#036>ğŸ† Achieved a perfect match on 47 out of 66 test images, demonstrating a Levenshtein score of 0 and underscoring the model's robustness.</li><li style=color:#036>âš–ï¸ Attained a Cross Entropy Loss of just 0.0005 on the training dataset for the decoder's next-token-prediction task, indicating high predictive accuracy.</li><li style=color:#036>ğŸ” Currently developing an attention mask using output_state and BertViz for the decoder side to enhance insight into the attention mechanisms within the model.</li></ul></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://github.com/azhara001 rel=me title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=https://www.linkedin.com/in/azhara001/ rel=me title=Linkedin data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a style=border-bottom:none href=mailto:abdullah_azhar@berkeley.edu rel=me title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2024 <a href=https://azhara001.github.io/>Abdullah Azhar</a></span>
<span>â€¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>