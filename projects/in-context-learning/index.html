<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models | Abdullah Azhar</title>
<meta name=keywords content><meta name=description content="Keywords - Deep Learning, LLMs, GPT2, In-Context-Learning, Pre-Training, PyTorch, Decision Trees, Gaussian Distribution, Noisy Training, Out-of-Distribution, Prompting, Root-Mean-Squared-Error"><meta name=author content="Abdullah Azhar"><link rel=canonical href=https://azhara001.github.io/projects/in-context-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.549e408c1cdbdced3c2cac56525067992a513db204d11fd97bfa6070cf92a519.css integrity="sha256-VJ5AjBzb3O08LKxWUlBnmSpRPbIE0R/Ze/pgcM+SpRk=" rel="preload stylesheet" as=style><link rel=icon href=https://azhara001.github.io/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://azhara001.github.io/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://azhara001.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://azhara001.github.io/favicon.png><link rel=mask-icon href=https://azhara001.github.io/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://azhara001.github.io/projects/in-context-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q603T56FWT"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q603T56FWT")}</script><meta property="og:title" content="Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models"><meta property="og:description" content="Keywords - Deep Learning, LLMs, GPT2, In-Context-Learning, Pre-Training, PyTorch, Decision Trees, Gaussian Distribution, Noisy Training, Out-of-Distribution, Prompting, Root-Mean-Squared-Error"><meta property="og:type" content="article"><meta property="og:url" content="https://azhara001.github.io/projects/in-context-learning/"><meta property="og:image" content="https://azhara001.github.io/favicon.png"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-06-05T14:18:23-06:00"><meta property="article:modified_time" content="2024-06-05T14:18:23-06:00"><meta property="og:site_name" content="Abdullah Azhar"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://azhara001.github.io/favicon.png"><meta name=twitter:title content="Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models"><meta name=twitter:description content="Keywords - Deep Learning, LLMs, GPT2, In-Context-Learning, Pre-Training, PyTorch, Decision Trees, Gaussian Distribution, Noisy Training, Out-of-Distribution, Prompting, Root-Mean-Squared-Error"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://azhara001.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models","item":"https://azhara001.github.io/projects/in-context-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models","name":"Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models","description":"Keywords - Deep Learning, LLMs, GPT2, In-Context-Learning, Pre-Training, PyTorch, Decision Trees, Gaussian Distribution, Noisy Training, Out-of-Distribution, Prompting, Root-Mean-Squared-Error","keywords":[],"articleBody":" [ Preprint, Github, Motivation] üîç Explored the ability of GPT2 Custom Configured Transformer Architecture (22M Params) to in-context learn noisy decision tree algorithm üß± Built upon the research conducted by garg2023transformers by incorporating:\na) Random Quadrant Prompting, b) Train-Test Overlapping Prompting for noise-free trained checkpoints on decision trees üß™ Additionally, tested the model's robustness by training it with i.i.d. Gaussian noise at standard deviations of 0, 1, and 3 Choice of Parameters: Batch Size: 64 Learning Rate: 1e-4 Tree Depth: 4 Number of Dimensions: 8 In-Context Examples: 40 ‚è≥ Ongoing project due to computational demands required to train a transformer model from scratch and the novelty of the issue. Readers are encouraged to review the preprint for preliminary results. ","wordCount":"119","inLanguage":"en","datePublished":"2024-06-05T14:18:23-06:00","dateModified":"2024-06-05T14:18:23-06:00","author":{"@type":"Person","name":"Abdullah Azhar"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://azhara001.github.io/projects/in-context-learning/"},"publisher":{"@type":"Organization","name":"Abdullah Azhar","logo":{"@type":"ImageObject","url":"https://azhara001.github.io/favicon.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://azhara001.github.io/ accesskey=h title="Abdullah Azhar (Alt + H)"><img src=https://azhara001.github.io/favicon.png alt="Site icon in header" aria-label=logo height=35>Abdullah Azhar</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://azhara001.github.io/about/ title=About><span>About</span></a></li><li><a href=https://azhara001.github.io/education/ title=Education><span>Education</span></a></li><li><a href=https://azhara001.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://azhara001.github.io/publications-and-preprints/ title=Publications><span>Publications</span></a></li><li><a href=https://azhara001.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://azhara001.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://azhara001.github.io/projects/>Projects</a></div><h1 class=post-title>Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models</h1></header><div class=post-content>[ <a href=preprint.pdf style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Preprint</i></ins></a>, <a href=https://github.com/azhara001/in-context-learning-trees style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Github</i></ins></a>, <a href="Adapting to Context- A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models.pdf" style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>Motivation</i></ins></a>]<div style="background-color:#e6e6fa;padding:10px;border:1px solid #ccc;margin:10px 0;position:relative;border-radius:20px"><ul><li style=color:#036>üîç Explored the ability of GPT2 Custom Configured Transformer Architecture (22M Params) to in-context learn noisy decision tree algorithm</li><li style=color:#036>üß± Built upon the research conducted by <a href=https://arxiv.org/abs/2208.01066 style=color:#800020 target=_blank rel="noopener noreferrer"><ins><i>garg2023transformers</i></ins></a> by incorporating:<br>a) Random Quadrant Prompting, b) Train-Test Overlapping Prompting for noise-free trained checkpoints on decision trees</li><li style=color:#036>üß™ Additionally, tested the model's robustness by training it with i.i.d. Gaussian noise at standard deviations of 0, 1, and 3</li><li style=color:#036>Choice of Parameters:<ul><li>Batch Size: 64</li><li>Learning Rate: 1e-4</li><li>Tree Depth: 4</li><li>Number of Dimensions: 8</li><li>In-Context Examples: 40</li></ul><li style=color:#b60442>‚è≥ Ongoing project due to computational demands required to train a transformer model from scratch and the novelty of the issue. Readers are encouraged to review the preprint for preliminary results.</li></ul></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://github.com/azhara001 rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=https://www.linkedin.com/in/azhara001/ rel=me title=Linkedin data-proofer-ignore><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a style=border-bottom:none href=mailto:abdullah_azhar@berkeley.edu rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://azhara001.github.io/>Abdullah Azhar</a></span>
<span>‚Ä¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>